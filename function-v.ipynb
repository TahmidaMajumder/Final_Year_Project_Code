{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12eb82ec",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-28T02:52:09.464792Z",
     "iopub.status.busy": "2025-01-28T02:52:09.464305Z",
     "iopub.status.idle": "2025-01-28T02:52:10.384279Z",
     "shell.execute_reply": "2025-01-28T02:52:10.383004Z"
    },
    "papermill": {
     "duration": 0.925633,
     "end_time": "2025-01-28T02:52:10.386498",
     "exception": false,
     "start_time": "2025-01-28T02:52:09.460865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Convert into datatime index\n",
    "def prepare_datetime_index(df, date_column='Date'):\n",
    "    \"\"\"Convert date column to datetime and set as index.\"\"\"\n",
    "    df = df.copy()\n",
    "    df[date_column] = pd.to_datetime(df[date_column])\n",
    "    df = df.drop_duplicates(subset=[date_column], keep='last')\n",
    "    df.set_index(date_column, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def impute_missing_dates(df):\n",
    "    \"\"\"Handle missing dates and duplicates in the time series.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Create full date range\n",
    "    full_range = pd.date_range(start=df.index.min(), end=df.index.max(), freq='D')\n",
    "    \n",
    "    # Reindex to include missing dates\n",
    "    df = df.reindex(full_range)\n",
    "    \n",
    "    # Fill missing values using forward fill\n",
    "    df = df.ffill()\n",
    "    \n",
    "    # Drop any duplicates and sort by index\n",
    "    df = df[~df.index.duplicated(keep='last')].sort_index()\n",
    "    \n",
    "    return df\n",
    "\n",
    "#Normal AQI count Function\n",
    "def calculate_aqi(data, subindex_columns=None):\n",
    "    \"\"\"Calculate Air Quality Index based on available pollutants.\"\"\"\n",
    "    data = data.copy()\n",
    "    \n",
    "    # Determine available pollutant columns\n",
    "    if subindex_columns is None:\n",
    "        all_possible_columns = ['co', 'no2', 'o3', 'pm10', 'pm25', 'so2']\n",
    "        subindex_columns = [col for col in all_possible_columns if col in data.columns]\n",
    "    \n",
    "    # Skip if no pollutant columns are available\n",
    "    if not subindex_columns:\n",
    "        print(f\"Warning: No pollutant columns available for {data.index}. Skipping record.\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate AQI as maximum value among available pollutants\n",
    "    data[\"AQI\"] = data[subindex_columns].max(axis=1)\n",
    "    data[\"AQI\"] = data[\"AQI\"].round()\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def handle_aqi_outliers(series, method='iqr'):\n",
    "    \"\"\"Handle outliers in AQI data.\"\"\"\n",
    "    if method == 'iqr':\n",
    "        Q1 = series.quantile(0.25)\n",
    "        Q3 = series.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        return series.clip(lower=lower_bound, upper=upper_bound)\n",
    "    elif method == 'rolling':\n",
    "        return series.rolling(window=7, center=True, min_periods=1).median()\n",
    "    return series\n",
    "\n",
    "\n",
    "def calculate_smooth_aqi(df):\n",
    "    \"\"\"Calculate smoothed AQI based on available pollutants.\"\"\"\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Get available pollutant columns\n",
    "    pollutant_columns = [col for col in ['co', 'no2', 'o3', 'pm10', 'pm25', 'so2'] \n",
    "                        if col in df.columns]\n",
    "    \n",
    "    if not pollutant_columns:\n",
    "        raise ValueError(\"No pollutant columns available for AQI calculation\")\n",
    "    \n",
    "    # Preprocess available components\n",
    "    for col in pollutant_columns:\n",
    "        df_processed[col] = handle_aqi_outliers(df_processed[col], method='iqr')\n",
    "        df_processed[col] = (df_processed[col]\n",
    "                           .rolling(window=6, center=True)\n",
    "                           .mean()\n",
    "                           .ffill()\n",
    "                           .bfill())\n",
    "    \n",
    "    # Calculate smooth AQI\n",
    "    smooth_aqi = df_processed[pollutant_columns].max(axis=1)\n",
    "    smooth_aqi = handle_aqi_outliers(smooth_aqi, method='rolling')\n",
    "    \n",
    "    df_processed['AQI_Smooth'] = smooth_aqi\n",
    "    \n",
    "    return df_processed"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4.238005,
   "end_time": "2025-01-28T02:52:11.008431",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-28T02:52:06.770426",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
